{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptracking.topic.lda_tomoto import *\n",
    "from ptracking.predict import Dataset\n",
    "from ptracking.sentiment.corenlp import sentiment, ner\n",
    "from ptracking.database.database import Fetcher\n",
    "from ptracking.twitter_scraper.twitter_scraper import TwitterFetcher\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "skf = StratifiedKFold()\n",
    "tss = TimeSeriesSplit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _ = tomoto_topics(30,30, gvmt_period='second')\n",
    "sentiments = sentiment()\n",
    "named_ent = ner()\n",
    "same_day = Fetcher().number_of_petitions_on_same_day()\n",
    "twitter = TwitterFetcher().get_twitter_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = topics.join(Dataset().prepare(columns=[\"created_at\"]))\n",
    "dataset.sort_values(\"created_at\", inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "X = np.array(dataset.iloc[:,1:-3].values.tolist())\n",
    "y = np.array(dataset['class'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with timesplit cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline + Sentiments Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = topics.join(sentiments)\n",
    "\n",
    "dataset = data.join(Dataset().prepare(columns=[\"created_at\"]))\n",
    "dataset.sort_values(\"created_at\", inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "X = np.array(dataset.iloc[:,1:-3].values.tolist())\n",
    "y = np.array(dataset['class'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with timesplit cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Sentiments + NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline + Sentiments + NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = topics.join(sentiments).join(ner)\n",
    "\n",
    "dataset = data.join(Dataset().prepare(columns=[\"created_at\"]))\n",
    "dataset.sort_values(\"created_at\", inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "X = np.array(dataset.iloc[:,1:-3].values.tolist())\n",
    "y = np.array(dataset['class'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with timesplit cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Sentiments + NER + Petitions on same day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline + Sentiments + NER + Petitions on same day Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = topics.join(sentiments).join(ner).join(same_day)\n",
    "\n",
    "dataset = data.join(Dataset().prepare(columns=[\"created_at\"]))\n",
    "dataset.sort_values(\"created_at\", inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "X = np.array(dataset.iloc[:,1:-3].values.tolist())\n",
    "y = np.array(dataset['class'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with timesplit cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Sentiments + NER + Petitions on same day + Debate count + Google trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline + Sentiments + NER + Petitions on same day + Debate count Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = topics.join(sentiments).join(ner).join(same_day).join(debate_count).join(trends)\n",
    "\n",
    "dataset = data.join(Dataset().prepare(columns=[\"created_at\"]))\n",
    "dataset.sort_values(\"created_at\", inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "X = np.array(dataset.iloc[:,1:-3].values.tolist())\n",
    "y = np.array(dataset['class'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with timesplit cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline + Sentiments + NER + Petitions on same day + Debate count + Google trends + Twitter features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Baseline + Sentiments + NER + Petitions on same day + Debate count + Twitter features Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = topics.join(sentiments).join(ner).join(same_day).join(debate_count).join(trends).join(twitter)\n",
    "\n",
    "dataset = data.join(Dataset().prepare(columns=[\"created_at\"]))\n",
    "dataset.sort_values(\"created_at\", inplace=True)\n",
    "dataset = dataset.reset_index()\n",
    "\n",
    "X = np.array(dataset.iloc[:,1:-3].values.tolist())\n",
    "y = np.array(dataset['class'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with stratified cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classifiers trained with timesplit cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  knn.fit(X_train, y_train)\n",
    "  y_pred = knn.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(knn, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = ComplementNB()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  nb.fit(X_train, y_train)\n",
    "  y_pred = nb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(nb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight='balanced')\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  svc.fit(X_train, y_train)\n",
    "  y_pred = svc.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(svc, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  dt.fit(X_train, y_train)\n",
    "  y_pred = dt.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(dt, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  rf.fit(X_train, y_train)\n",
    "  y_pred = rf.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(rf, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = XGBClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  gb.fit(X_train, y_train)\n",
    "  y_pred = gb.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(gb, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "mcc_scores = list()\n",
    "acc_scores = list()\n",
    "f1_scores = list()\n",
    "\n",
    "for train_index, test_index in tss.split(X, y):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  mlp.fit(X_train, y_train)\n",
    "  y_pred = mlp.predict(X_test)\n",
    "  mcc_scores.append(round(matthews_corrcoef(y_test,y_pred),2))\n",
    "  acc_scores.append(round(accuracy_score(y_test,y_pred),2))\n",
    "  f1_scores.append(round(f1_score(y_test,y_pred, average='weighted'),2))\n",
    "  ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)\n",
    "\n",
    "print(\"MCC\", round(np.mean(mcc_scores),2))\n",
    "print(\"Accuracy\", round(np.mean(acc_scores),2))\n",
    "print(\"F1\", round(np.mean(f1_scores),2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a98825134ff8ede45dc7c86b38b3b5fbe3144690d83619ea9060570621c6e61"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
